<p>The <code>robots.txt</code> file is one of the primary ways of telling a search engine where it can and can't go on your website. All major search engines support the basic functionality it offers. There are some extra rules that are used by a few search engines which can be useful too. This guide covers all the uses of <code>robots.txt</code> for your website. While it looks deceivingly simple, making a mistake in your <code>robots.txt</code> can seriously harm you site, so make sure to read and understand this.</p><p><ul class="toc">
</li></ul></p><h2 id="what-is-robots-txt">What <em>is</em> a <code>robots.txt</code> file?</h2><p><section class="alignright extra" id=""><h4><i class="fa fa-question-circle"></i> humans.txt</h4><p>A couple of developers sat down and realized that they were, in fact, not robots. They were (and are) humans. So they created the <a href="http://humanstxt.org/Standard.html">humans.txt standard</a> as a way of highlighting which people work on a site, amongst other things.</p>
</section>A <code>robots.txt</code> file is a text file, following a strict syntax. It's going to be read by search engine spiders. These spiders are also called robots, hence the name. The syntax is strict simply because it has to be computer readable. There's no reading between the lines here, something is either 1, or 0.</p><p>Also called the "Robots Exclusion Protocol", the <code>robots.txt</code> file is the result of a consensus between early search engine spider developers. It's not an official standard by any standards organization, but all major search engines do adhere to it.</p><h2 id="what-does-robots-txt-do">What does the <code>robots.txt</code> file <em>do</em>?</h2><p><section class="alignright extra" id=""><h4><i class="fa fa-info-circle"></i> Crawl directives</h4></section>The <code>robots.txt</code> file is one of a few crawl directives. We have guides on all of them, find them here:</p><p><a href="https://yoast.com/tag/crawl-directives/" target="_blank">Crawl directives guides by Yoast »</a>[/aside]Search engines index the web by spidering pages. They follow links to go from site A to site B to site C and so on. Before a search engine spiders <em>any</em> page on a domain it hasn't encountered before, it will open that domains <code>robots.txt</code> file. The <code>robots.txt</code> file tells the search engine which URLs on that site it's allowed to index.</p><p>A search engine will cache the <code>robots.txt</code> contents, but will usually refresh it multiple times a day. So changes will be reflected fairly quickly.</p><p><img class="noborder alignnone wp-image-724703 size-full" src="https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI.png" alt="robots.txt" width="1200" height="628" /></p><h2 id="robots-txt-location">Where should I put my <code>robots.txt</code> file?</h2><p>The <code>robots.txt</code> file should always be at the root of your domain. So if your domain is www.example.com, it should be found at <code>http://www.example.com/robots.txt</code>. Do be aware: if your domain responds without www. too, make sure it has the same <code>robots.txt</code> file! The same is true for http and https. When a search engine wants to spider the URL <code>http://example.com/test</code>, it will grab <code>http://example.com/robots.txt</code>. When it wants to spider that same URL but over https, it will grab the robots.txt from your https site too, so <code>https://example.com/robots.txt</code>.</p><p>It's also very important that your <code>robots.txt</code> file is really called <code>robots.txt</code>. The name is case sensitive. Don't make any mistakes in it or it will just not work.</p><h2 id="robots-txt-pros-cons">Pros and cons of using <code>robots.txt</code></h2><h3 id="robots-txt-crawl-budget">Pro: crawl budget</h3><p>Each site has an "allowance" in how many pages a search engine spider will crawl on that site, SEOs call this the crawl budget. By blocking sections of your site from the search engine spider, you allow your crawl budget to be used for other sections. Especially on sites where a lot of SEO clean up has to be done, it can be very beneficial to first quickly block the search engines from crawling a few sections.</p><p><section class="alignright extra" id=""><h4><i class="fa fa-info-circle"></i> blocking query parameters</h4></section>One situation where crawl budget is specifically important is when your site uses a lot of query string parameters to filter and sort. Let's say you have 10 different query parameters and with different values, that can be used in any combination. This leads to hundreds if not thousands of possible URLs. Blocking all query parameters from being crawled will help make sure the search engine only spiders your site's main URLs and won't go into the enormous trap that you'd otherwise create.</p><p>This line would block all URLs on your site with a query string on it:</p><pre>Disallow: /*?*</pre><p>[/aside]</p><h3 id="robots-txt-remove-from-search-results">Con: not removing a page from search results</h3><p>Using the <code>robots.txt</code> file you can tell a spider where it cannot go on your site. You can <em>not</em> tell a search engine which URLs it cannot show in the search results. This means that not allowing a search engine to crawl a URL - called "blocking" it - does not mean that URL will not show up in the search results. If the search engine finds enough links to that URL, it will include it, it will just not know what's on that page.</p><p><img class="alignnone size-full wp-image-718101" src="https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/blocked-directory.png" alt="Screenshot of a result for a blocked URL in the Google search results" width="525" height="61" /></p><p>If you want to reliably block a page from showing up in the search results, you need to use a meta robots <code>noindex</code> tag. That means the search engine has to be able to index that page and find the <code>noindex</code> tag, so the page should <em>not</em> be blocked by <code>robots.txt</code>.</p><h3 id="robots-txt-not-spreading-link-value">Con: not spreading link value</h3><p>Because the search engine can't crawl the page, it cannot distribute the link value for links to your blocked pages. If it could crawl, but not index the page, it could still spread the link value across the links it finds on the page. When a page is blocked with <code>robots.txt</code>, the link value is lost.</p><h2 id="robots-txt-syntax"><code>robots.txt</code> syntax</h2><p><section class="alignright extra" id=""><h4><i class="fa fa-wordpress"></i> WordPress <code>robots.txt</code></h4></section><br /> We have a complete article on how to best setup your <a href="https://yoast.com/wordpress-robots-txt-example/"><code>robots.txt</code> for WordPress</a>. Note that you can edit your site's <code>robots.txt</code> file in the Yoast SEO Tools → File editor section.[/aside]A <code>robots.txt</code> file consists of one or more blocks of directives, each started by a user-agent line. The "user-agent" is the name of the specific spider it addresses. You can either have one block for all search engines, using a wildcard for the user-agent, or specific blocks for specific search engines. A search engine spider will always pick the most specific block that matches its name.</p><p>These blocks look like this (don't be scared, we'll explain below):</p><pre>User-agent: *
Disallow: /

User-agent: Googlebot
Disallow:

User-agent: bingbot
Disallow: /not-for-bing/</pre><p>Directives like <code>Allow</code> and <code>Disallow</code> should not be case sensitive, so whether you write them lowercase or capitalize them is up to you. The values <em>are </em>case sensitive however, <code>/photo/</code> is not the same as <code>/Photo/.</code> We like to capitalize directives for the sake of readability in the file.</p><p><section class="alignright extra show-off"><p>Order a <span>website review</span> and get a plugin of your choice for free. We'll even configure it for you</p>	<form id="edd_purchase_173931" class="edd_download_purchase_form edd_purchase_173931" method="post">


		<div class="edd_purchase_submit_wrapper">
			<input type="submit" class="edd-add-to-cart edd-no-js button orange edd-submit" name="edd_purchase_download" value="&#036; 699&nbsp;&ndash;&nbsp;Buy now &raquo;" data-action="edd_add_to_cart" data-download-id="173931" data-variable-price="no" data-price-mode=single /><a href="https://yoast.com/checkout/" class="edd_go_to_checkout button orange edd-submit" style="display:none;">Checkout</a>
															</div><!--end .edd_purchase_submit_wrapper-->

		<input type="hidden" name="download_id" value="173931">
							<input type="hidden" name="edd_action" class="edd_action_input" value="add_to_cart">

					<input type="hidden" name="edd_redirect_to_checkout" id="edd_redirect_to_checkout" value="1">

		<!-- Including template "/nas/content/live/yoastcom/wp-content/themes/yoast-v5/html_includes/shop/purchase-link.php" -->
<button type="submit" class="edd-add-to-cart edd-no-js" name="edd_purchase_download" value="Buy this bundle now" data-action="edd_add_to_cart" data-download-id="173931">&#036; 699&nbsp;&ndash;&nbsp;Buy now &raquo;</button>

	</form><!--end #edd_purchase_173931-->
<div class="alignright"><small><a href="https://yoast.com/hire-us/website-review/">More info &raquo;</a></small></div><img class="hide-on-mobile" src="https://yoast-mercury.s3.amazonaws.com/uploads/2010/08/Review_Banner.png" alt="Get a Yoast website review"/></section></p><h3 id="robots-txt-user-agent"><code>User-agent</code> directive</h3><p>The first bit of every block of directives is the user-agent. A user-agent identifies a specific spider. The user-agent field is matched against that specific spider's (usually longer) user-agent. For instance, the most common spider from Google has the following user-agent:</p><pre><code>Mozilla/5.0 (compatible; Googlebot/2.1;
  +http://www.google.com/bot.html</code>)</pre><p>A relatively simple <code>User-agent: Googlebot</code>  line will do the trick if you want to tell this spider what to do.</p><p>Note that most search engines have multiple spiders. They will use specific spiders for their normal index, for their ad programs, for images, for videos, etc.</p><p>Search engines will always choose the most specific block of directives they can find. Say you have 3 sets of directives: one for <code>*</code>, one for <code>Googlebot</code> and one for <code>Googlebot-News. </code>If a bot comes by whose user-agent is <code>Googlebot-Video</code>, it would follow the <code>Googlebot</code> restrictions. A bot with the user-agent <code>Googlebot-News</code> would use the more specific <code>Googlebot-News</code> directives.</p><h4 id="list-common-user-agents">The most common user agents for search engine spiders</h4><p>Below is a list of the user-agents you can use in your <code>robots.txt</code> file to match the most commonly used search engines:</p><table class="full-width"><thead><tr><th>Search engine</th><th>Field</th><th>User-agent</th></tr></thead><tbody><tr><td>Baidu</td><td>General</td><td><code>baiduspider</code></td></tr><tr><td>Baidu</td><td>Images</td><td><code>baiduspider-image</code></td></tr><tr><td>Baidu</td><td>Mobile</td><td><code>baiduspider-mobile</code></td></tr><tr><td>Baidu</td><td>News</td><td><code>baiduspider-news</code></td></tr><tr><td>Baidu</td><td>Video</td><td><code>baiduspider-video</code></td></tr><tr><td>Bing</td><td>General</td><td><code>bingbot</code></td></tr><tr><td>Bing</td><td>General</td><td><code>msnbot</code></td></tr><tr><td>Bing</td><td>Images & Video</td><td><code>msnbot-media</code></td></tr><tr><td>Bing</td><td>Ads</td><td><code>adidxbot</code></td></tr><tr><td>Google</td><td>General</td><td><code>Googlebot</code></td></tr><tr><td>Google</td><td>Images</td><td><code>Googlebot-Image</code></td></tr><tr><td>Google</td><td>Mobile</td><td><code>Googlebot-Mobile</code></td></tr><tr><td>Google</td><td>News</td><td><code>Googlebot-News</code></td></tr><tr><td>Google</td><td>Video</td><td><code>Googlebot-Video</code></td></tr><tr><td>Google</td><td>AdSense</td><td><code>Mediapartners-Google</code></td></tr><tr><td>Google</td><td>AdWords</td><td><code>AdsBot-Google</code></td></tr><tr><td>Yahoo!</td><td>General</td><td><code>slurp</code></td></tr><tr><td>Yandex</td><td>General</td><td><code>yandex</code></td></tr></tbody></table><h3 id="robots-txt-disallow"><code>Disallow</code> directive</h3><p>The second line in any block of directives is the <code>Disallow</code> line. You can have one or more of these lines, specifying parts of the site the specified spider can't access. An empty <code>Disallow</code> line means you're not disallowing anything, so basically it means that spider can access all sections of your site.</p><pre>User-agent: *
Disallow: /</pre><p>The example above would block all search engines that "listen" to <code>robots.txt</code> from crawling your site.</p><pre>User-agent: *
Disallow:</pre><p>The example above would, with only one character less, <em>allow </em>all search engines to crawl your entire site.</p><pre>User-agent: googlebot
Disallow: /Photo</pre><p>The example above would block Google from crawling the <code>Photo</code> directory on your site and everything in it. This means all the subdirectories of the <code>/Photo</code> directory would also not be spidered. It would <em>not</em> block Google from crawling the <code>photo</code> directory, as these lines are case sensitive.</p><h3 id="robots-txt-regexes">How to use wildcards / regular expressions</h3><p>"Officially", the robots.txt standard doesn't support regular expressions or wildcards. However, all major search engines do understand it. This means you can have lines like this to block groups of files:</p><pre>Disallow: /*.php
Disallow: /copyrighted-images/*.jpg</pre><p>In the example above, <code>*</code> is expanded to whatever filename it matches. Note that the rest of the line is still case sensitive, so the second line above will not block a file called <code>/copyrighted-images/example.JPG</code> from being crawled.</p><p>Some search engines, like Google, allow for more complicated regular expressions. Be aware that not all search engines might understand this logic. The most useful feature this adds is the <code>$</code>, which indicates the end of a URL. In the following example you can see what this does:</p><pre>Disallow: /*.php$</pre><p>This means <code>/index.php</code> could not be indexed, but <code>/index.php?p=1</code> <em>could</em> be indexed. Of course, this is only useful in very specific circumstances and also pretty dangerous: it's easy to unblock things you didn't actually want to unblock.</p><h3 id="robots-txt-non-standard-directives">Non-standard <code>robots.txt</code> crawl directives</h3><p>On top of the <code>Disallow</code> and <code>User-agent</code> directives there are a couple of other crawl directives you can use. These directives are not supported by all search engine crawlers so make sure you're aware of their limitations.</p><h4 id="robots-txt-allow"><code>Allow</code> directive</h4><p>While not in the original "specification", there was talk of an <code>allow</code> directive very early on. Most search engines seem to understand it, and it allows for simple, and very readable directives like this:</p><pre>Disallow: /wp-admin/
Allow: /wp-admin/admin-ajax.php</pre><p>The only other way of achieving the same result without an <code>allow</code> directive would have been to specifically <code>disallow</code> every single file in the <code>wp-admin</code> folder.</p><h4 id="robots-txt-noindex"><code>noindex</code> directive</h4><p>One of the lesser known directives, Google actually supports the <code>noindex</code> directive. We think this is a very dangerous thing. If you want to keep a page out of the search results, you usually have a good reason for that. Using a method of blocking that page that will only keep it out of Google, means you leave those pages open for other search engines. It could be very useful in a specific Googlebot user agent bit of your <code>robots.txt</code> though, if you're working on improving your crawl budget. Note that <code>noindex</code> isn't officially supported by Google, so while it works now, it might not at some point.</p><h4 id="robots-txt-host"><code>host</code> directive</h4><p>Supported by Yandex (and not by Google even though some posts say it does), this directive lets you decide whether you want the search engine to show <code>example.com</code>  or <code>www.example.com</code>. Simply specifying it as follows does the trick:</p><pre>host: example.com</pre><p>Because only Yandex supports the <code>host</code> directive, we wouldn't advise you to rely on it. Especially as it doesn't allow you to define a scheme (http or https) either. A better solution that works for all search engines would be to 301 redirect the hostnames that you <em>don't</em> want in the index to the version that you <em>do</em> want. In our case, we redirect www.yoast.com to yoast.com.</p><h4 id="crawl-delay"><code>crawl-delay</code> directive</h4><p>Supported by Yahoo!, Bing and Yandex the <code>crawl-delay</code> directive can be very useful to slow down these three, sometimes fairly crawl-hungry, search engines. These search engines have slightly different ways of reading the directive, but the end result is basically the same.</p><p>A line as follows below would lead to Yahoo! and Bing waiting 10 seconds after a crawl action. Yandex would only access your site once in every 10 second timeframe. A semantic difference, but interesting to know. Here's the example <code>crawl-delay</code> line:</p><pre>crawl-delay: 10</pre><p>Do take care when using the <code>crawl-delay</code> directive. By setting a crawl delay of 10 seconds you're only allowing these search engines to index 8,640 pages a day. This might seem plenty for a small site, but on large sites it isn't all that much. On the other hand, if you get 0 to no traffic from these search engines, it's a good way to save some bandwidth.</p><h4 id="sitemap-urls"><code>sitemap</code> directive for XML Sitemaps</h4><p>Using the <code>sitemap</code> directive you can tell search engines - specifically Bing, Yandex and Google - the location of your XML sitemap. You can, of course, also submit your XML sitemaps to each search engine using their respective webmaster tools solutions. We, in fact, highly recommend that you do. Search engine's webmaster tools programs will give you very valuable information about your site. If you don't want to do that, adding a <code>sitemap</code> line to your <code>robots.txt</code> is a good quick option.</p><p><p class="readmore"><a title="several articles about Webmaster Tools" data-prefix="Read more" href="https://yoast.com/tag/webmaster-tools/">Read more: &lsquo;several articles about Webmaster Tools&rsquo; &raquo;</a></p></p><h2 id="robots-txt-validate">Validate your <code>robots.txt</code></h2><p>There are various tools out there that can help you validate your <code>robots.txt</code>, but when it comes to validating crawl directives, we like to go to the source. Google has a <code>robots.txt</code> testing tool in its Google Search Console (under the Crawl menu) and we'd highly suggest using that:</p><p><img class="alignnone size-large wp-image-717273" src="https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/robots-txt-tester-600x508.png" alt="robots.txt tester" width="600" height="508" /></p><p>Be sure to test your changes thoroughly before you put them live! You wouldn't be the first to accidentally <code>robots.txt-</code>block your entire site into search engine oblivion.</p><p><p class="readmore"><a title="WordPress robots.txt example for great SEO" data-prefix="Keep reading" href="https://yoast.com/wordpress-robots-txt-example/">Keep reading: &lsquo;WordPress robots.txt example for great SEO&rsquo; &raquo;</a></p></p><img width="266" height="139" src="https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI-600x314.png" class="attachment-266x266 size-266x266" alt="robots.txt" srcset="https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI-600x314.png 600w, https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI-250x131.png 250w, https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI-768x402.png 768w, https://yoast-mercury.s3.amazonaws.com/uploads/2016/05/Robots_FI.png 1200w" sizes="(max-width: 266px) 100vw, 266px">
